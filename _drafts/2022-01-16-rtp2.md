---
layout: post
title: Aliasing
subtitle: Ray Tracing Journey - Pt2
type: blog
permalink: raytracing-part-two
usemathjax: true

aliasingpost: "[[2]](#aliasing-in-computer-graphics)"
---

* TOC
{:toc}

# 1. Introduction

I'll keep this intro short and sweet. This post picks up where the [first](https://joshortner.github.io/raytracing-part-one) raytracing post left off. I'll be starting at section 7, Antialiasing, in [RTIOW](https://raytracing.github.io/books/RayTracingInOneWeekend.html#antialiasing).

# 2. Aliasing 

## 2.1 Tangent Warning

I was first introduced to the concept of aliasing pretty early in my graphics jouney, as I imagine most are. The moment you want to draw something more complicated than a perfectly vertical or horizontal line, you'll notice an artifact coloquially referred to as "jaggies."

When I'm learning something new that is complex, it's easy for me to loose sight of the fundamentals, the first-principles underlying the idea. This was definitely the case when I first started learning computer graphics. CG is a beast. It's unlike anything else I had learned in computer science up to that point, and I constantly found myself getting deep into the weeds and loosing sight of the big picture. 

Until I understand the links that an idea has to fundamentals, I don't feel like I truly understand it. Reading RTIOW, put me back in contact with the problem of aliasing and with out the tools of graphics apis to solve it. I realized I was lacking knowldge of the link between the problem of aliasing and the fundamentals. 

But what are the fundamentals underlying the problem of aliasing? That would be the field of digital signal processing. 

Computer graphics is really a problem of digital signal processing. The fundamental question is: how do we represent continuous data discretely? Additionally there's the whole problem of projection because 3D data has to be represented on a 2D surface. 

Before I go on, I want to say that I'm introducing a huge topic here that will assume some knowledge from the reader. I'm in no place to teach much of that knowledge, nor is that what this post is about. Instead I want to provide an account of my journey through this information, explain some of my newly gained intuition on the subject, and provide resources for others interested in diving deep into the details. 

Raytracing doesn't require that you dig this deep so this post is definitely a tangent. All you really need to know to continue with RTIOW is that aliasing can cause visual artifacts that may be undesired, and there are exisiting solutions the problem. Pick one to solve it in the context of your applicaiton.

It's interesting what you can learn when you take a deep dive into one topic. I started wanting to learn a bit about the causes of aliasing, I came away with a better understanding of Digital Signal processing, Fourier analysis, and the sampling theorem as well as an entirely new understanding of what a pixel is.

## Im not an expert. Check out my sources to gain your own understanding of the information.

## What is Digital Signal Processing?

> "It is the mathematics, the algorithms, and the techniques used to manipulate ... signals after they have been converted into a digital form" - [DSP Chapter 1](https://www.dspguide.com/ch1/1.htm).

Digital Signal Processing (DSP) is an area of computer science dedicated to developing tools to analyze *signals*. 

What's a signal? So many things... For the most part, it refers to data generated by the real world. Think, "light, sound, seismic vibrations". DSP provides methods for analyzing these signals in a wide variety of contexts: from space to medical to commercial. But we don't have to understand the entire field of DSP to gain insight into why aliasing occurs in computer graphics. In fact, we need only look at one particular step in the DSP pipeline; it just so happens to be probably the most important steps in the entire process: Analog-to-Digital Conversion. 

*Signals* (not always, but mostly always) are continuous in nature: "voltage that varies over time; a chemical reaction rate that depends on temperature" [DSP, Ch3: ADC and DAC](http://www.dspguide.com/ch3.htm). Or perhaps the surface of an object in 3D space define by an equation, like the equation a sphere centered around $$(Cx, Cy, Cz)$$: 

$$(x - Cx)^2 + (y - Cy)^2 + (z - Cz)^2 = R^2$$
{: style="text-align: center;" }

Before a computer can do any kind of reasoning about a signal, it has to be converted to a form that the computer can understand. In practice this is often done with specialized hardware, however digging into the mathematics of this process can give us the answers we're looking for as well as a deeper understanding of the graphics pipeline.

### Analog-to-Digital Conversion

For an analog signal to be converted to a digital form it has to be *sampled* and then *quantized*. These processes **remove** information from the original source, so the name of the game here is in understanding what information has to be retained and what information can be lost.

#### Sampling

Sampling is the process of obtaining an instantaneous value of an input signal at periodic intervals[DSP, Ch3: ADC and DAC](http://www.dspguide.com/ch3.htm). This process converts the independent variable of a signal from continous to discrete. This is important so I'll repeat it: ***Sampling converts the independent variable of a signal from continuous to discrete.***

Take a simple, one dimensional example. A sine wave. The formula of the particular wave below is: $$y = 2\sin{(2x)}$$

<img src="{{site.url}}/public/media/rt/rtp2/simple-sine.png" alt="drawing" width="50%" style="margin: 0 auto"/>

We'll say the independent variable $$x$$ represents some unit of time, and the dependent variable $$y$$ represents some measured value. $$y$$ could be something like amplitude, and $$x$$ could be seconds, but we'll keep units abstract for now.

If we sample the signal at every $$\frac{1}{4} \mathrm{time unit}$$, we get the following:

<img src="{{site.url}}/public/media/rt/rtp2/sampled-sine.png" alt="drawing" width="50%" style="margin: 0 auto"/>

The independent time variable has been converted to discrete points. These green points along the wave are our sampled dataset. The loss of information in the sampling process are the values on the function between the points.

The output of the sampling process becomes the input to the quantization process. 

#### Quantization

So we've descritized the independent variable, however the dependent variable is still continuous and can take on real numbers potentially more precise than a computer can represent. If we had an infinite number of bits to work with than there'd probably be no need for quantization, but in practice we are always restricted by the capabilities of our hardware, so we have to decide on a method to map the continuous dependent variable to a valid integer or float [Aliasing in Computer Graphics](https://apoorvaj.io/aliasing-in-computer-graphics/). This is *quantization*.

Quantization converts a continuous dependent variable into a discrete one. <br>
Again: ***Quantizing converts a continuous dependent variable into a discrete one.***


- https://www.cs.princeton.edu/courses/archive/fall00/cs426/papers/hanrahan95.pdf

---


The domain and range of the sample space is the viewport, which is defined by the parameters of the simulated camera. Defining the viewport, and sampling it are two separate processes. So we can look at one without the other. This post is about sampling so we'll assume a viewport has been defined.


In the context of computer graphics, we are interested in image processing, and it turns out that images are special in their characteristics compared to many other signals. 

1. Images are a measure of a parameter over space, not time like most signals 
2. They contain a huge amount of information:
    - To store one second of television video requires 10 megabytes while a similar length voice signal can be store with a thousand times less memory. 
3. The final judge of quality is subjective
[DSP, Ch1: Image Processing](http://www.dspguide.com/ch1/5.htm)

## what is sampling

## what is quantization

## aliasing

- https://technobyte.org/whats-aliasing-dsp-how-to-prevent-it/

## the fourier series

## the sampling therom

## 2.2 Digital Signal Processing

> "Digital signal processing (DSP) is the use of digital processing, such as by computers or more specialized digital signal processors, to perform a wide variety of signal processing operations. The digital signals processed in this manner are a sequence of numbers that represent samples of a continuous variable in a domain such as time, space, or frequency." - [wiki](https://en.wikipedia.org/wiki/Digital_signal_processing)

One of the most fundamental underlying questions asked by computer graphics is: how do we convert the continuous 3D data of a scene into a discrete 2D form. 

In fact, there's an entire field dedicated to formulating methods of converting continous data into discrete data, while preserving as much information as possible. 

[American Scientist](https://www.americanscientist.org/article/a-pixel-is-not-a-little-square)

The second sentence in the above quote is the most important. In computer graphics, the continuous data is a model, in what ever form that takes. In the case of raytracing a sphere it's the ray-sphere intersection formula described in RTIOW. In the case of the traditional rasterization pipeline, it's a triangle mesh. The samples are the pixels (and this was very new to me). 

In computer graphics, pixels are samples. To be honest, thinking of pixels this way was brand new to me when I first heard it. 

**A pixel is a sample point**

It's not a square, which is my natural interpretation [A pixel is not a little square](http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf). The reason it's intuitive to interpret a pixel as a square in computer graphics has to do with the reconstruction filter (a box filter) used in CG, but not because of some fundamental quality of a pixel. In fact, there are many different kinds reconstruction filters with differing geometric interpretations. So when looking at the math, a pixel is just a sample point.

The basic goal of a reconstruction filter is to smooth the output of a analogue-to-digital converter [reconstruction filter](https://www.sciencedirect.com/topics/engineering/reconstruction-filter). So in the context of raytracing, there are functions in the scene that define the continous surfaces of objects, and we sample the scene a points along a 2D surface (the viewport). Then a box reconstruction filter is used to fill the color of the space between pixel samples.

So why are pixels sample points? The process of converting continuous data into a form that can be represented by a computer involves two steps [image processing book](https://sisu.ut.ee/imageprocessing/book/2):

1. Sampling 
2. Quantization

### 2.2.1 Sampling

### 2.2.2 Quantization


---

I found myself wanting to learn more about the fundamental origin of the issue, and I was delighted to fing that 

I first learned about aliasing pretty early on in my graphics journey. I'd imagine it's the same for most people, as "jaggies" are visible the moment you draw anything that isn't a straight line. I learned that it's a side effect of using pixels to generate images, and it could mostly be handled with some OpenGL setting (TO DO: Look into this, I don't remember how I solved it before, blend?). When I got to the anti aliasing section in RTIOW, I realized I hadn't given much thought to the details of the phenomenon. So, I started doing more research and fell into a mildly deep rabbit hole and gain some really valuable insight into how to think about computer graphics in a fundamental way, and a better understanding of digital signal processing which, as an aspiring Computer Scientist, is something I should be spending more time on. 

There is a really great blog post describing in detail, why aliasing occurs in computer graphics. Check it out [here](https://apoorvaj.io/aliasing-in-computer-graphics/). I'm definitely not going to try and rehash the details here. Instead, I'll explain some of the intuition I gained from reading it.

The first thing I learned is that Aliasing is related to the larger field of signal processing. 

Raytracing can be thought of as sampling a continuous space at descrete intervals. The samples are taken at pixel centers, so the sampling period is the distance between pixel centers{{ page.aliasingpost }}, which is just the size of the pixel.

A pixel is not a square, it is a point sample: [A pixel is not a little square](http://alvyray.com/Memos/CG/Microsoft/6_pixel.pdf).

"An image is a rectilinear array of point samples (pixels). The marvelous Sampling Theorem tells us that we can reconstruct a continuous entity from such a discrete entity using an appropriate reconstruction filter 3"

"If the Nyquist rate of the output of this function is greater than our pixel frequency, then aliasing will occur."{{ page.aliasingpost }}

To me, growing as a Computer Scientist means filling gaps of knowledge when I'm faced with one. When I ask myself, "why does aliasing occur," I realize there's a gap. So I spent some time filling the gap with some signal processing information.

Digital image processing is the process of converting continous data, into a form that can be represented by a computer. This is done in two steps: 
1. Sampling
2. Quantization

"The sampling rate determines the spatial resolution of the digitized image, while the quantization level determines the number of grey levels in the digitized image" [image processing book](https://sisu.ut.ee/imageprocessing/book/2)


https://courses.engr.illinois.edu/ece110/sp2021/content/courseNotes/files/?samplingAndQuantization



# . References

<a id="RTIOW">[1]</a>
[RTIOW](https://raytracing.github.io/books/RayTracingInOneWeekend.html)
{: style="font-size: 90%; text-align: left; margin: 0px;"}

<a id="aliasing-in-computer-graphics">[2]</a>
[Aliasing in computer graphics](https://apoorvaj.io/aliasing-in-computer-graphics/)
{: style="font-size: 90%; text-align: left; margin: 0px;"}